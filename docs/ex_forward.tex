\subsection{Forward Propagation}
The Loss $= Loss(w,b)$ in linear regression is the \textit{Square Error}:
\begin{align*}
  L_i(\vec{w}, b) &= (y_i - \hat{y})^2\\
  &=(y_i -\vec{w}\cdot{}\vec{x}_j -b)^2
\end{align*}

In Linear Regression, the cost is the averaged sum of $L_i$, and it's the \textit{Mean Square Error}:
\begin{align}
  C(w_1, w_2, b) &= \frac{1}{2} \sum_{i=0}^{i=2} L_i(w_1, w_2, b)\\
  &= \frac{1}{2}([y_1, y_2] - [\hat{y}_1, \hat{y}_2])\cdot{}([y_1, y_2]-[\hat{y}_1, \hat{y}_2])\\
  &=\frac{1}{2}([y_1, y_2] - [w_1, w_2]\mathbf{X}-b)\cdot{}([y_1, y_2] - [w_1,w_2]\mathbf{X} -b) \label{cost}
\end{align}
$\frac{1}{2}$ is to average over examples. Equation \ref{cost} is what we implement into code.
Outliers may be critical on the effect of weights and biases.

In Python it'd look like:
\begin{center}
  \begin{verbatim}
  Y' = np.dot(w,X) - b
  cost = 1/2*np.pow(2,(Y-Y'))
  \end{verbatim}
\end{center}





\subsection{Backward Propagation}
We need the Cost's derivative to find better weights and bias. The complicated thing is keeping track of each step.
\begin{align}
  dC(\vec{w},b) &= \frac{dC}{d\vec{w}} + \frac{dC}{db}\nonumber\\
  &= d(\sum_{i=0}^{i=2}L_i) = \sum_{i=0}^{i=2}dL_i(\vec{w},b) \label{diffp}\\
  &=\sum_{i=0}^{i=2} \frac{dL_i}{d\vec{w}} + \frac{dL}{db}\nonumber
\end{align}
Equation \ref{diffp} makes use of linearity property of diff. We sum over columns (each sample). Now, pick up a column/sample $k$:
\begin{align*}
  L_k(\vec{w},b) &= (y_k - \hat{y_k})^2\\
    L_k &= A_k(w_1, w_2, b)^2\\
    dL_k &= 2\cdot{}A_k(w1,w2,b)\cdot{}dA_k
\end{align*}
We named $A_k$ to the difference $y_i-\hat{y}_i$.
So $dC$ was reduced to:
\begin{align}
  dC &= \frac{1}{2}\sum_{i=0}^{i=2} dL_i\\
  &= \frac{1}{2}\times{}2\cdot{}A_i(w_1, w_2, b)\cdot{}dA_i
\end{align}
Where $A_i$ will be broadcasted, the shape is $1\times{}m$ and the sape of $dA_i$ is $n\times{}m$. Right now, $m=2$. The next step will be to sum over colums. And although $A_i$ will be calculated ($y_i - \hat{y}_i$) in the code, we need it now to find $dA_k$:
\begin{align*}
  dL_k  &= 2A_k\cdot{}(\frac{dA_k}{dw_1} + \frac{dA_k}{dw_2} + \frac{dA}{db}) \\
  A_k &= A_k(w_1, w_2, b)\\
  &= y_k - \vec{w}\cdot{}\vec{x_k} - b\\
  &= y_k - w_1\cdot{}x_{1k} - w_2\cdot{}x_{2k}-b\\
  dA_k &= -x_{1k} - x_{2k} -1
\end{align*}
where 
\begin{align*}
 \frac{dA_{k}}{dw_1} &= -x_{1k}\\ 
  \frac{dA_{k}}{dw_2} &= -x_{2k}\\ 
  \frac{dA_k}{db} &= -1
\end{align*} 

This can be expressed in compact form:
\begin{align}
  dC = \frac{1}{2}\times{}sum(2A\times(-\mathbf{X} -1))
\end{align}
Where ``sum'' implies we sum all values from the resulting array, and $b$ will be broadcast to the shape of $\mathbf{X}$.

We also found each partial derivative:
\begin{align}
  dC &= \frac{dC}{dw} + \frac{dC}{db} \\
 dC &= -sum(2A\times{}\mathbf{X}) - sum(2A\times{}1)
\end{align}
To update the parameters, \textit{Gradient Descent} method is used. We update the vector $\vec{w}$ as follows:
\begin{align}
  \vec{w} &= \vec{w} -\frac{dC}{dw}\cdot{}\alpha\\
  \vec{b} &= \vec{b} -\frac{dC}{db}\cdot{}\alpha
\end{align}
$\alpha$ is called \textit{learning rate} and we use to tune the derivation. We go against the gradient so the sign is changed to $-$ (it points to max incresing direction otherwise).
