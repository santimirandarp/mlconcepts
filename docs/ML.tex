\documentclass[14pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} % for using & and align formulas
\usepackage{parskip} % new paragraph adds a blank line before
\setlength{\parindent}{0in} % no indent
\renewcommand{\familydefault}{\sfdefault} %change font to ss
\usepackage{graphicx} %include images

\begin{document}
\include{Intro}
\section{Example: Multivariable Linear Regression}

A linear regression model for 2 features:
\begin{align*}
 y &= w_1\cdot x_1 + w_2\cdot x_2 + b \\
   &= \vec{w}\cdot\vec{x} + b
\end{align*}
$w_1$,$w_2$ control the slopes of this plane, $b$ translates it up and down. $\vec{x}$ is for one sample. Many samples are represented as a matrix:
\begin{align}
[y1, y2] = 
  [w1, w2]\cdot{}
  \begin{bmatrix}
  x_1 & x_1\\
  x_2 & x_2 
  \end{bmatrix}
 +  b
\end{align}
$y_i$ is the result for each sample (columns in the matrix).

We need to do forward and backward propagation.
\include{ex_forward}
\subsection{General Derivation}
The problem is to find $w_i, b$ such that the multidimensional "plane" has small error respect to each datapoint. Then for a new datapoint we will have a trained predictor.

In linear regression, the model is:
\begin{align*}
 y &= w_1\cdot x_1 + w_2\cdot x_2 +\ldots+ w_n\cdot x_n\\
   &= \sum_i^n w_i\cdot x_i + b \\
   &= \vec{w}\cdot\vec{x} + b
\end{align*}
Here $\vec{x}$ is for one sample. For $n$ samples, it becomes a matrix, we write $\vec{y} = \vec{w}\cdot\mathbf{X} + \vec{b}$. This is represented:
\begin{equation}
  [y_1 y_2 \ldots y_n] = 
  [w_1 w_2 \ldots w_n] \cdot
  \begin{bmatrix}
    x_{11} & x_{12} & \ldots & x_{1m}\\
    x_{21} & x_{22} & \ldots & x_{2m}\\
    \vdots & \vdots & \ddots & \vdots\\
    x_{n1} & x_{22} & \ldots & x_{nm}\\
  \end{bmatrix}
  + [b_1 b_2 \ldots b_n]
\end{equation}
There are $m$ examples-columns with $n$ features-rows. Hence $[\mathbf{X}] = m\times{}n$

The Loss $= L(w,b)$ in linear regression is the \textit{Square Error}:
\begin{align*}
  L(\vec{w},b) &= (y - \hat{y})^2\\
  &= (y_k - \vec{w}_j\cdot \vec{x}_j - b)^2
\end{align*}
The cost in any method/model measures how well it's doing with he current parameters. In Linear Regression, it is the averaged sum of $L_k$, and it's the \textit{Mean Square Error}:
\begin{align}
  C(\vec{w}, \vec{b}) &= \frac{1}{m}\sum_{i=0}^m L_i(\vec{w}, b)\\
  &=\frac{1}{m}\sum_{i=0}^m (\vec{y}_i - \vec{\hat{y}}_i)\cdot{}(\vec{y}_i - \vec{\hat{y}}_i)\\
  &= \frac{1}{m} (\vec{y} - \vec{w}\mathbf{X} - \vec{b})\cdot{}(\vec{y} - \vec{w}\mathbf{X} - \vec{b})
\end{align}
$C(\vec{w})$ is a way to denote $C$ depends on all the variables in $\vec{w}$. \textit{m} is the number of samples.

The cost is a ``bowl'', we will reach the global minimum (or close).
We use the Cost derivative to find the better weight and bias. 

\begin{align*}
  dC(\vec{w}, \vec{b}) &= \frac{dC}{dw} + \frac{dC}{db}\\
  &= \frac{1}{m} d(\sum^m_0 L_i)\\
  &= \frac{1}{m} \sum^m_{i=0} dL_i(\vec{w}, b_i)\\
  &= \frac{1}{m} \sum^m_{i=0} \frac{dL_i}{d\vec{w}} + \frac{dL_i}{db}\\
  &= \frac{1}{m} \sum^m_{i=0} \frac{dL_i}{dw_1} +\ldots +\frac{dL_i}{dw_n} + \frac{dL}{db}
\end{align*}

Take a particular column $k$:
\begin{align*}
  dL_k &= 2A * (\frac{dA}{dw_1}+ \frac{dA}{dw_2}+ \frac{dA}{db})\\
  A_k &= A(w_1, \ldots, w_n, b)\\
      &= y_k - \vec{w}\cdot{}\vec{x}_k - b \\
      &= y_k - w_1\cdot{}x_{1k}-\ldots{}-w_n\cdot{}x_{nk} - b \\
  dA_k &=  -x_{1k} -x_{2k} - x_{nk} -1 
\end{align*}
There is no graphical justification as to why we update $w$ and $b$ like that, but we are moving $w$ against (minus) the gradient multiplied by a constant (alpha) called \textit{learning rate}.

The minus sign is because the gradient always points away from the minimum and we want towards it (in one dimension there are only 2 directions). 

The process is called \textit{Gradient Descent}.

Because the $y_d - y_p$ is squared, MSE is a parabola for $b$ and $w$ then it makes sense: the farther away we are from the minimum the larger the gradient, and the more we want to update $w$ and $b$.






\section{Summary}

\begin{itemize}
 \item Model/Architecture*: equation to fit the data,
 \item FordP: Calculate the Cost, Evaluates the error,
 \item BackP: Training. Use cost, update $w$, $b$.
\end{itemize}
* Technically, model is only once the parameters are defined (after training). 
\begin{center}
Model =  Architecture + parameters (fixed ones). 
\end{center}

FP means from $X$ to $C$; BP from $L$ to $z$, in \textit{terms of the chain rule}.
\include{Other}
\end{document}
