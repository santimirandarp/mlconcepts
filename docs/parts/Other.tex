\section{Related Concepts}
\textit{Overfitting}: Occurs when the cost in the training dataset decreases but it increases on the test dataset. The model starts to \textit{memorize} data, and does not \textit{generalize}.
\begin{center}
\includegraphics[width=0.5\textwidth]{overfitting.png}
\end{center}

Training error: blue; validation error: red, both as a function of the number of training cycles. The best predictive and fitted model would be where the validation error has its global minimum.

\textit{Underfitting}: It occurs when the model or algorithm does not fit the data enough. It could be a bad model (too simple, or just not the right fit), or a lack of training, etc.

\textit{Classification v Regression}: A classification model is one which attempts to predict a class, or category. That is, it's predicting from a number of discrete possibilities, such as "dog" or "cat." A regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location. Which one we use depends on the nature of the variables.

\textit{Cross Validation}: The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias, and to give an insight on how the model will generalize to an independent dataset.

Why a CNN? It's the current state-of-the-art approach to creating computer vision models.

\textit{Cost vs Metric}

Cost is the function we use to improve the model by gradient descent. We also compare the cost for training set vs cost for the test set, and look for the minimum so there is no overfitting.

But for reporting a number measuring how well the model has done there is the Metric. It could be the cost, or not. In the case of linear regression the cost is MSE, but the Metric normally used is RMSE so the result has same units than $y$.
