\section{Example: Multivariable Linear Regression}
A linear regression model for 2 features:
\begin{align*}
  a(x_1, x_2) &= w_1\,x_1 + w_2\,x_2 + b \\
   &= \vec{w}\cdot\vec{x} + b
\end{align*}
$w_1$,$w_2$ control the slopes of this plane, $b$ translates it up and down. $\vec{x}$ is for one sample. Two or more samples are represented as a matrix:
\begin{align*}
[a_1, a_2] = 
  [w_1, w_2]\cdot{}
  \begin{bmatrix}
  x_1 & x_1\\
  x_2 & x_2 
  \end{bmatrix}
 +  b
\end{align*}
$a_i$ is the result for each sample (columns in the matrix).

The same than for a line (figure \ref{fig:line}) by tuning $W$ and $b$  we can find the best linear fit. The sign of $W$ reflects the line over Y($a$) axis, and its magnitude controls the slope; $b$ translates the line over $x$. Without $b$ the line has to go through the origin. 

We need to do forward and backward propagation.
\begin{figure}[h]
 \centering
 \includegraphics[width=0.9\textwidth]{line_plot.png}
  \caption{Line plot} \label{fig:line}
\end{figure}


\subsection{Forward Propagation}
The Loss denoted $= Loss(w_1,\ldots, w_n, b)$ or just $L(\vec{w},b)$ in linear regression is the \textit{Square Error}:
\begin{align*}
  L_i(\vec{w}, b) &= (a_i - \hat{a})^2\\
  &=(a_i -\vec{w}\cdot{}\vec{x}_{i} -b)^2
\end{align*}
$\vec{w}\cdot{}\vec{x}_i$ can also be denoted $\sum_jw_j\cdot{}\mathbf{X}_{ji}$. In the first form we multiply the vector $w$ and the column $i$ (dot product).

In Linear Regression, the Cost is the averaged sum of $L_i$, and it's the \textit{Mean Square Error}:
\begin{align}
  C(w_1, w_2, b) &= \frac{1}{2} \sum_{i=0}^{i=2} L_i(w_1, w_2, b)\nonumber\\
  &= \frac{1}{2}([a_1, a_2] - [\hat{a}_1, \hat{a}_2])\cdot{}([a_1, a_2]-[\hat{a}_1, \hat{a}_2])\nonumber\\
  &=\frac{1}{2}([a_1, a_2] - [w_1, w_2]\cdot{}\mathbf{X}-b)\cdot{}([a_1, a_2] - [w_1,w_2]\cdot{}\mathbf{X} -b) \label{cost}
\end{align}
$\frac{1}{2}$ is to average over examples. Equation \ref{cost} is what we implement into code.
%Outliers may be critical on the effect of weights and biases.

In Python it'd look like:
\begin{center}
  \begin{BVerbatim}
  Ap = np.dot(w,X) - b
  diff = A-Ap 
  cost = 1/2*np.dot(diff, diff.T)
  \end{BVerbatim}
\end{center}





\subsection{Backward Propagation}
\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{derivative.png}
  \caption{Derivative}\label{fig:basics}
\end{figure}

For a function of a single variable $C(x)$ the derivative at a point $x_0$ is the slope (figure \ref{fig:basics}) at $x_0$.

For many variables, it's a similar process: the total derivative of a function is the sum of partial derivatives.

\begin{align}
  dC(\vec{w},b) &= \frac{\partial C}{\partial w_1}+ \frac{\partial C}{\partial w_2} + \frac{\partial C}{\partial b}\\
\end{align}
$C(w,b)$ derivative is used to find better weights and bias:
\begin{align*}
  \vec{w} &= [w_1, w_2] - [\frac{\partial C}{\partial w_1},\frac{\partial C}{\partial w_2}]\cdot{}\alpha\\
  \vec{w} &= \vec{w} -\frac{\partial C}{\partial \vec{w}}\cdot{}\alpha\\
  b &= b -\frac{\partial C}{\partial b}\cdot{}\alpha
\end{align*}
Let's get started.
\begin{align}
  dC(\vec{w},b) &= \frac{\partial C}{\partial w_1}+ \frac{\partial C}{\partial w_2} + \frac{\partial C}{\partial b}\\
  &= d(\sum_{i=0}^{i=2}L_i) = \sum_{i=0}^{i=2}dL_i(\vec{w},b) \label{diffp}\\
  &=\sum_{i=0}^{i=2} \frac{\partial L_i}{\partial w_1} +\frac{\partial L_i}{\partial w_2} + \frac{\partial L}{\partial b}
\end{align} 
These equations are important. Equation \ref{diffp} makes use of linearity property of diff.

We sum over columns (each sample). Now, pick up a column/sample $k$:
\begin{align*}
  L_k(\vec{w},b) &= (a_k - \hat{a}_k)^2\\
    L_k &= A_k(w_1, w_2, b)^2\\
    dL_k &= 2\,A_k(w1,w2,b)\,dA_k
\end{align*}
$A_k$ is the difference $a_k-\hat{a}_k$. We need $dw$ and $db$; and for this $dA_k$:
\begin{align*}
  dL_k  &= 2A_k\,(\frac{\partial A_k}{\partial w_1} + \frac{\partial A_k}{\partial w_2} + \frac{\partial A_k}{\partial b}) \\
  A_k &= A_k(w_1, w_2, b)\\
  &= a_k - \vec{w}\cdot{}\vec{x_k} - b\\
  &= a_k - w_1\,x_{1k} - w_2\,x_{2k}-b\\
  dA_k &= -x_{1k} - x_{2k} -1
\end{align*}
where 
\begin{center}
\begin{align*}
  \frac{\partial A_{k}}{\partial w_1} = -x_{1k}\hspace{2em} \frac{\partial A_{k}}{\partial w_2} = -x_{2k}\hspace{2em} \frac{\partial A_k}{\partial b} = -1
\end{align*} 
\end{center}

This can be expressed in compact form:
\begin{align*}
  dC &= sum(-\frac{1}{2}\times{}2\mathbf{X}\cdot{}\vec{A^T}-\frac{1}{2}\times{}2\vec{A}\times{}\vec{1})\\
  &= \frac{\partial C}{\partial \vec{w}} + \frac{\partial C}{\partial b} 
\end{align*}

To update the parameters, \textit{Gradient Descent} method is used. We update the vector $\vec{w}$ as follows:
\begin{align}
  \vec{w} &= \vec{w} -\frac{\partial C}{\partial \vec{w}}\,\alpha\nonumber\\
  &= \vec{w} +\frac{1}{2}\times{}2\mathbf{X}\cdot{}\vec{A^T}\,\alpha\\ 
  b &= b -\frac{\partial C}{\partial b}\cdot{}\alpha\nonumber\\
  &= b +\frac{1}{2}\times{}2\vec{A}\times{}\vec{1}\,\alpha
\end{align}
$\alpha$ is called \textit{learning rate} and we use to tune the derivation. We go against the gradient so the sign is changed to $-$ (it points to max incresing direction otherwise).

In deep neural networks $b$ will explicitly be a vector.

\subsubsection{Interpretation}

There is no graphical justification as to why we update $w$ and $b$ like that, but we are moving $w$ against (minus) the gradient multiplied by a constant (alpha) called \textit{learning rate}.

The minus sign is because the gradient always points away from the minimum and we want towards it (in one dimension there are only 2 directions). 

\begin{align}
  \vec{w} &= \vec{w} -\frac{\partial C}{\partial \vec{w}}\,\alpha\\
  &= \vec{w} -[\frac{\partial C}{\partial w_1}, \frac{\partial C}{\partial w_2},\ldots, \frac{\partial C}{\partial w_n}]\,\alpha\\
\end{align}

It means we have a vector of corrections for each slope such that the overall cost is decreased. If we take say $\frac{dC}{dw1}$ it is the sum of slopes for each sample, divided by $m$. 

To have more insight and detail, we could take a look at a dataset with just one feature. Then 
\begin{align*}
  C &= \frac{1}{m}\sum_i(a_i - \hat{a}_i)^2 \\ 
  \frac{\partial C}{dw_1}&= \frac{1}{m}\,\sum_i \frac{dL_i}{dw_1} \\
  &= -\frac{2}{m}\sum_i \mathbf{X}_{1i}(\vec{a}_i-\vec{\hat{a}}_i)
\end{align*}
we see the slope depends on the sum of features times errors. Much more can be inspected here.

%This is almost the same result we get in the \textit{sigmoid}, just multiplied by $2$. Because the $2$ can be thought as inside the learning rate, we can use the exact same \textit{backpropagation} for both methods!

\section{Linear Regression: General Derivation}
Only main differences are shown as the process is the same.

The problem is to find $w_i$, $b$ such that the multidimensional ``plane'' has small error respect to each datapoint. Then for a new datapoint we will have a trained predictor.

In linear regression, the model is:
\begin{align*}
 a &= w_1\, x_1 + w_2\, x_2 +\ldots+ w_n\, x_n\\
   &= \sum_i^n w_i\, x_i + b \\
   &= \vec{w}\cdot\vec{x} + b
\end{align*}
Here $\vec{x}$ is for one sample. For $n$ samples, it becomes a matrix, we write $\vec{y} = \vec{w}\cdot\mathbf{X} + \vec{b}$. This is represented:
\begin{equation*}
  [a_1, a_2, \ldots, a_n] = 
  [w_1, w_2, \ldots, w_n] \cdot
  \begin{bmatrix}
    x_{11} & x_{12} & \ldots & x_{1m}\\
    x_{21} & x_{22} & \ldots & x_{2m}\\
    \vdots & \vdots & \ddots & \vdots\\
    x_{n1} & x_{22} & \ldots & x_{nm}\\
  \end{bmatrix}
  + [b_1, b_2, \ldots, b_m]
\end{equation*}
The $b_i$ are all the same number. There are $m$ examples-columns with $n$ features-rows. Hence $[\mathbf{X}] = m\times{}n$

