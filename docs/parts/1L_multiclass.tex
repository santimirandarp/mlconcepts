\section{Multiclass Network}

This network can be used for multiclass problems, but it's actually used with a \textit{softmax} function instead of \textit{sigmoid}. 

The prediction for a sample ($a$) is a column vector. The cost can be arranged as a row vector of $N$ components, for $N$ different labels.

From the NN diagram, we derive the computations:

\begin{equation*}
  \begin{bmatrix}
    a_{11} & a_{12}& \ldots& a_{1m}\\ 
    a_{12} & a_{22}& \ldots& a_{2m}\\ 
    \vdots & \vdots & \vdots& \vdots\\ 
    a_{s1} & a_{s2}& \ldots& a_{sm} 
  \end{bmatrix}
    =\sigma( 
  \begin{bmatrix}
    w_{11} & w_{12}& \ldots& w_{1n}\\ 
    w_{12} & w_{22}& \ldots& w_{2n}\\ 
    \vdots & \vdots & \vdots& \vdots\\ 
    w_{sn} & w_{sn}& \ldots& w_{sn} 
  \end{bmatrix}
  \cdot{}
  \begin{bmatrix}
    x_{11} & x_{12} & \ldots & x_{1m}\\
    x_{21} & x_{22} & \ldots & x_{2m}\\
    \vdots & \vdots & \ddots & \vdots\\
    x_{n1} & x_{22} & \ldots & x_{nm}\\
  \end{bmatrix}
  +
  \begin{bmatrix}
    b_1\\ b_2\\ \vdots\\ b_s
  \end{bmatrix})
\end{equation*}
$s$ dimension is equal to the number of nodes in the output layer, and $n$ to the number of features on the input layer. $\mathbf{B}$ will actually be broadcasted to match the shape of $\mathbf{A}$. 

The cost for the $j$ node is:
\begin{align}
  C_j = -\frac{1}{m}\left(\sum_i \mathbf{A}_{ji}\log(\hat{\mathbf{A}}_{ji}) + (1-\mathbf{A}_{ji})\log(1-\hat{\mathbf{A}}_{ji})\right) 
\end{align}
which may be coded like this:
\begin{verbatim}
C = np.sum(
         multiply(A, np.log(Ap))
       + multiply(1-A, np.log(1-Ap)),
       axis=1)
\end{verbatim}
The gradients are: 
\begin{align}
  \frac{dC}{d\mathbf{W}} &= -\frac{1}{m}\mathbf{\Delta A}\cdot{}\mathbf{X}^T\\
  \frac{dC}{d\mathbf{B}} &= -\frac{1}{m}\mathbf{\Delta A}
\end{align}

$\mathbf{A}$ was a vector, now it's turned into a matrix, one row for each node. Same thing for $\mathbf{W}$.
