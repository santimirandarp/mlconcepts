\section{Sigmoid}
The derivation is similar to Linear regression. The plot for the equation:
\[
  \hat{a}(\vec{x})= \frac{1}{1+e^{-(\vec{w}\cdot{}\vec{x}+b)}}
\]

for one dimension is on figure \ref{fig:sigmoid}. By tuning $w$ and $b$  we can find the best fit. 

The sign of $w$ reflects the line over $y$ axis, the magnitude controls the slope. Again, $b$ translates over $x$.

\begin{figure}[h]
 \centering
 \includegraphics[width=0.9\textwidth]{sigmoid_plot.png}
  \caption{Sigmoid. Mistake: at $x=0$, $y=(e^{-b}+1)^{-1}$} \label{fig:sigmoid}
\end{figure}

\subsection{Forward Propagation}
\begin{itemize}
  \item The Loss is the Cross Entropy Loss,
  \item The activation is \textit{sigmoid}.
\end{itemize}

\begin{align}
  \hat{a}_i &= \frac{1}{1+e^{-(\vec{w}\cdot \vec{x}_i + b)}}\\
  L_i &= a_i\,\log(\hat{a}_i) + (1-a_i)\,\log(1-\hat{a}_i)
\end{align}
$\vec{w}\cdot{}\vec{x}_i$ is shorthand for $\sum_{j} w_j\,\mathbf{X}_{ji}$ (dot product for vector-matrix)

\textbf{Cost}
\begin{align*}
  C(\vec{w}, \vec{b}) &= -\frac{1}{m}\sum_{i=0}^m L_i(\vec{w}, b)
\end{align*}

This is coded:
\begin{verbatim}
cost = 1/m*(np.dot(A, np.log(Ap).T) 
+ np.dot(1-A, np.log(1-Ap).T))
#or
Cost = 1/m*(np.sum(np.multiply(A, np.log(Ap)) 
+ np.multiply(1-A, np.log(1-Ap))))
#tested
\end{verbatim}

\subsection{Backward propagation}

\begin{align}
  \frac{\partial L_i}{\partial w_j} = 
  \left(\frac{\partial L_i}{\partial \hat{a}_i}\, \frac{\partial \hat{a}_i}{\partial z}\right)\,\frac{\partial z}{\partial w_j}\\
  \frac{\partial L_i}{\partial b} = 
  \left(\frac{\partial L_i}{\partial \hat{a}_i}\,\frac{\partial \hat{a}_i}{\partial z}\right)\,\frac{\partial z}{\partial b}
\end{align}
The first two derivatives (enclosed in parens) are the same, let's compute them.

\begin{align*}
  \frac{\partial L_i}{\partial \hat{a}_i} &= \frac{a_i}{\hat{a}_i} + \frac{(1-a_i)}{1-\hat{a}_i}\,(-1) \\
%  \frac{\partial \hat{a}_i}{\partial z_i} &= \ldots  \label{eq:derZ}\\
%  a(z)&= \frac{1}{1+e^{-z}}\\ we can apply parts if we want
%  a(u)&=\frac{1}{u}\\
%  da&=-u^{-2}\,du\\
  \frac{d\hat{a}_i}{dz}&=-\left(\frac{1}{1+e^{-z}}\right)^{-2}\,e^{-z}(-1)\\
  &=\left(\frac{1}{1+e^{-z}}\right)^{-2}\,(e^{-z} + 1 -1)\\
  &= \hat{a}^2\,(\frac{1}{\hat{a}} -1) \\
  \frac{\partial \hat{a}_i}{\partial z} &= \hat{a}_i\,(1-\hat{a}_i)
\end{align*}
Hence the derivative is:
\begin{align*}
  \frac{\partial L_i}{\partial \hat{a}_i}\,\frac{\partial \hat{a}_i}{\partial z_i} &= \left[ \frac{a_i}{\hat{a}_i} + \frac{(1-a_i)}{1-\hat{a}_i}\,(-1) \right]\,\hat{a}_i\,(1-\hat{a}_i)\\
  &= a_i - \hat{a}_i\\
  \frac{\partial z_i}{\partial w_j} &= \mathbf{X}_{ji}\\
  \frac{\partial z_i}{\partial b} &= 1
\end{align*}
We are left with:

\begin{align*}
  \frac{\partial L_i}{\partial \hat{a}_i}\,\frac{\partial \hat{a}_i}{\partial z_i}\,\frac{\partial z_i}{\partial w_j} &= \left(a_i - \hat{a}_i\right)\,\mathbf{X}_{ji}\\
  \frac{\partial L_i}{\partial \hat{a}_i}\,\frac{\partial \hat{a}_i}{\partial z_i}\,\frac{\partial z_i}{\partial b} &= \left(a_i - \hat{a}_i\right)\,1\\
\end{align*}

We then have:
\begin{align}
  w_j &= w_j - \frac{\partial C}{\partial w_j}\,\alpha\\
  &= w_j + \frac{1}{m}\,\sum \frac{\partial L_i}{\partial w_j}\,\alpha\label{eq:same}
\end{align}
But we can compute in general:
\begin{align}
  \vec{w} &= \vec{w} - \frac{\partial C}{\partial \vec{w}}\,\alpha\\
  \vec{w} &= \vec{w} - \mathbf{X}\cdot{}\mathbf{A}^T\,\alpha\\
  b &= b - \frac{\partial C}{\partial b}\,\alpha
\end{align}

And this is the exact same result than for linear regression. 
