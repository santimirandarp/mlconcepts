\section{Results}
After experimenting with single layer NNs, the following are a few conclusions.

In general, non-normalized data blows up some calculation.
In sigmoid, it is the exponential $wX+b$; in linear, because of $A^2$ in the cost. Here also the learning rate can be really large (up to 100) and the code optimizes well. With smaller learning rates, we need more cycles.

\subsection{Interesting}
The backward propagation ends up being the same for both methods (apart from a constant as far as I can see). So it is useful for both methods. The cost is different though.

