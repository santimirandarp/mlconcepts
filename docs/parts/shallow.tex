\section{Shallow Neural Network}
A shallow network adds a hidden layer. This helps to model more complex data patterns respect to standalone linear or logistic regression. The next calculations follow the network on figure \ref{fig:shallow}.

\subsection{Forward Propagation}
\begin{itemize}
  \item activation tanh hidden layer, 
  \item activation sigmoid output layer,
  \item cost is a number given by log-loss
\end{itemize}

$\mathbf{W}$, passes from a vector to a matrix. Each row will represent a node.

\begin{align}
  \mathbf{A}^{[i]} = \mathbf{W}^{[i]} \cdot{} \mathbf{A}^{[i-1]} + \mathbf{B}^{[i]} \label{eq:multi}
\end{align}

\begin{equation*}
  \begin{bmatrix}
    a_{11}^{[i]} &\ldots & a_{1m}^{[i]}\\
    \vdots &\ddots & \vdots\\
    a_{s1}^{[i]}  &\ldots & a_{sm}^{[i]}\\
  \end{bmatrix}
  = 
  \begin{bmatrix}
    w_{11}^{[i]}& \ldots& w_{1n}^{[i]}\\
    \vdots& \vdots& \ddots\\
    w_{s1}^{[i]}& \ldots& w_{sn}^{[i]}\\
  \end{bmatrix}
  \begin{bmatrix}
    a^{[i-1]}_{11} & \ldots & a^{[i-1]}_{1m}\\
    \vdots & \ddots & \vdots\\
    a^{[i-1]}_{n1} & \ldots & a^{[i-1]}_{nm}\\
  \end{bmatrix}
  + \begin{bmatrix}b_1^{[i]}\\ \vdots\\ b_s^{[i]}\end{bmatrix}
\end{equation*}

For any $\mathbf{A}$, each column correlates to a sample. Each row is always a node, for $\mathbf{{A}^{[i-1]}}$ this is the number of features.

And each output activation in the row, is a linear combination of weights and bias from that node.

\subsubsection{Why non-linear}
The activation in the hidden layer can not be linear. Take a hidden layer with two nodes, and two initial input features.

\begin{equation*}
  \begin{bmatrix}
    a_{11} & a_{12}\\
    a_{21} & a_{22}\\
  \end{bmatrix}
  = 
  \begin{bmatrix}
    w_{11}& w_{12}\\
    w_{21}& w_{22}
  \end{bmatrix}
  \begin{bmatrix}
    x_{11} & x_{12}\\
    x_{21} & x_{22} 
  \end{bmatrix}
  + \begin{bmatrix}b_1\\ b_2\end{bmatrix}
\end{equation*}

First row in the output matrix is for node $1$. First term is for first sample, second for second sample. Second row are computations for node $2$.

So the first column is:
\begin{align*}
  a_{11} &= w_{11}\,x_{11} + w_{12}\,x_{21} + b_1\\
  a_{21} &= w_{21}\,x_{11} + w_{22}\,x_{21} + b_2\\
\end{align*}
When is input to another linear function the result is 
\begin{align*}
  r_1 &= c_1\,a_{11} + c_2\,a_{21} + b_1\\
  &= c\,x_{12} + c'\,x_{21} + b_1
\end{align*}
This is the same than using a single linear neuron! The same thing happens if we a sigmoid. Hence \textit{linear functions aren't normally used in hidden layers.}

\subsection{Implement Forward Propagation}
In the code, we implement equation \ref{eq:multi} several times. The most important piece is:

\begin{center}
\begin{BVerbatim}
def fp(A0,m,nodes=[4,3,2,1],hfn=np.tanh,ofn=sigmoid):
    """
    A0, matrix of samples
    nodes: architecture, list of nodes per layer
    hfn: hidden layer function
    ofn: output layer function
    returns row vector/matrix of predictions
    """
    W,B = initialize(nodes[0], A0.shape[0]) 
    Ap = predict(W,B,A0,m,hfn) # nodesL x samples
    for l in range(len(nodes[1:-1])):
        W,B = initialize(nodes[l+1], nodes[l]) 
        Ap = predict(W,B,Ap,m,hfn) # nodes x samples 
    W,B = initialize(nodes[-1], Ap.shape[0]) #nodes x nodes_prev
    return predict(W,B,Ap,m,ofn)
\end{BVerbatim}
\end{center}
Sanity check: $W$, $b$ aren't initialized as zeros, but if they were and the output is a sigmoid, the result has to be $0.69$ (tested on scripts).

\subsection{Backward propagation}
The quantities to compute are:
\begin{itemize}
  \item C, dC
  \item dL
  \item $w'$, $b'$
\end{itemize}
