\section{High Level View}

On its core, Machine Learning about is a program that changes from exposure to data (experience).

Data and parameters are the input to a mathematical function. This outputs a prediction, that we later use to update the parameters. The process is iterative.

Deep Learning models complex patterns of data. It's particularly useful for non-linear patterns, and opens up a new space of problems to solve. Because Deep Learning is a part of Machine Learning, the previous logic is found deep network diagrams.

Examples of Network Diagrams can be found in section \ref{section:figs}. 

\subsection{The steps}
There are two main steps: \textit{forward} and \textit{backward} propagation. 

Suppose a mathematical function is given to us:
$$ \hat{y}(x_1,x_2) = a x_1 + b x_2 + c$$
where $a$, $b$, $c$ are \textit{parameters}.

In \textbf{Forward Propagation} we initialize a set of parameters (say $a,b,c=0$), and use the equation to estimate the real output ($y$). Both values are input to ``$C(y, \hat{y})$'' which is small if we're doing well, or large if bad. 

So forward propagation is the calculation of $\hat{y}$ and $C(y,\hat{y})$ whatever shape they happen to have. 

In \textbf{Backward Propagation} we minimize $C$, by differentiation, and find a way to move our function parameters towards the minimum of $C$. We use \textit{Gradient Descent} for this task.

The simplest neural network is the single layer. Later, hidden layers are included, from 1 (shallow network), to many (deep network).

We will see examples in detail, starting with multivariable linear regression, that is, a previous step to \textit{Deep Learning}.

\subsection{Summary}
Calculate $\hat{y}$ which is an estimation of $y$, compute $C(y,\hat{y})$, and use this to update $\hat{y}$ parameters, iteratively.

In the next chapters $y$, $\hat{y}$ are $a$ and $\hat{a}$; this is just the notation used in ML/DL.
